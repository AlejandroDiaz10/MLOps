{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77299245",
   "metadata": {},
   "source": [
    "\n",
    "# Equipo 34 — Fase 1 (Pipeline ML reproducible con DVC)\n",
    "\n",
    "**Rol:** ML Engineer — *Emilio Contreras*  \n",
    "**Dataset:** South German Credit (versión limpia `german_credit_cleaned_v1.csv`)  \n",
    "**Pipeline:** EDA → Preprocess → Train → Evaluate (scikit-learn) + **DVC**\n",
    "\n",
    "> Este notebook sirve como *orquestador demostrativo*: ejecuta (o muestra) las etapas clave y reúne evidencias para el PDF y el video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe7497",
   "metadata": {},
   "source": [
    "\n",
    "## ML Canvas — Credit Scoring (South German Credit)\n",
    "\n",
    "**Predicción objetivo**  \n",
    "Clasificar solicitantes como *good* / *bad* credit risk.\n",
    "\n",
    "**Usuarios / Stakeholders**  \n",
    "Analistas de riesgo, áreas de crédito, cumplimiento/regulación, clientes finales.\n",
    "\n",
    "**Decisiones**  \n",
    "Aprobación/rechazo; condiciones del préstamo (tasa, monto, garantías).\n",
    "\n",
    "**Fuentes de datos**  \n",
    "South German Credit (1,000 filas, 21 columnas). Variables demográficas, historial bancario, condiciones del préstamo.\n",
    "\n",
    "**Métricas de éxito**  \n",
    "AUC-ROC (principal), F1 (clase minoritaria), Recall (malos pagadores), matriz de confusión por grupo.\n",
    "\n",
    "**Restricciones / Riesgos**  \n",
    "Desbalanceo de clases; sesgo/ética (edad, género, estado civil); explicabilidad requerida.\n",
    "\n",
    "**Pipeline (alto nivel)**  \n",
    "EDA → Limpieza/Imputación → Codificación/Scaling → Split estratificado → Modelos (LogReg, RF, XGB) → Tuning → Interpretabilidad (SHAP) → Reporte.\n",
    "\n",
    "**Valor**  \n",
    "Reduce pérdidas por morosidad y mejora inclusión financiera con criterios consistentes y auditables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f0af8f",
   "metadata": {},
   "source": [
    "\n",
    "## Requisitos y estructura sugerida\n",
    "\n",
    "```bash\n",
    "pip install -U pandas numpy scikit-learn matplotlib seaborn dvc pyyaml joblib shap\n",
    "```\n",
    "\n",
    "```\n",
    ".\n",
    "├─ data/\n",
    "│  ├─ raw/\n",
    "│  │  └─ german_credit_cleaned_v1.csv\n",
    "│  └─ interim/\n",
    "├─ models/\n",
    "├─ reports/\n",
    "│  ├─ figures/\n",
    "│  └─ metrics.json\n",
    "├─ src/\n",
    "│  ├─ utils.py\n",
    "│  ├─ eda.py\n",
    "│  ├─ preprocess.py\n",
    "│  ├─ train.py\n",
    "│  └─ evaluate.py\n",
    "├─ params.yaml\n",
    "└─ dvc.yaml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parámetros clave del proyecto (ajusta si tu target tiene otro nombre)\n",
    "DATA_RAW = \"data/raw/german_credit_cleaned_v1.csv\"\n",
    "INTERIM = \"data/interim/clean.parquet\"\n",
    "MODEL_PATH = \"models/model.pkl\"\n",
    "REPORT_PATH = \"reports/metrics.json\"\n",
    "TARGET = \"credit_risk\"  # ajusta si no coincide con tu CSV\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e1b38",
   "metadata": {},
   "source": [
    "## EDA (resumen rápido en notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7ab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "df = pd.read_csv(DATA_RAW)\n",
    "display(df.head())\n",
    "display(df.describe(include='all').transpose().head(20))\n",
    "\n",
    "Path(\"reports/figures\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "num_cols = df.select_dtypes(include='number').columns.tolist()[:6]\n",
    "for c in num_cols:\n",
    "    ax = df[c].hist(bins=30)\n",
    "    ax.set_title(f\"Histograma - {c}\")\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig(f\"reports/figures/hist_{c}.png\", bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"Figuras guardadas en reports/figures/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a492acda",
   "metadata": {},
   "source": [
    "## Preprocesamiento (imputación, OHE, scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c2d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"Target '{TARGET}' no está en columnas: {list(df.columns)}\")\n",
    "\n",
    "y = df[TARGET]\n",
    "X = df.drop(columns=[TARGET])\n",
    "\n",
    "num_cols = X.select_dtypes(include='number').columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), num_cols),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_proc = pre.fit_transform(X)\n",
    "\n",
    "Path(\"data/interim\").mkdir(parents=True, exist_ok=True)\n",
    "np.save(INTERIM.replace(\".parquet\", \"_X.npy\"), X_proc)\n",
    "y.to_frame(name=TARGET).to_parquet(INTERIM.replace(\".parquet\", \"_y.parquet\"))\n",
    "joblib.dump(pre, \"models/preprocess.joblib\")\n",
    "\n",
    "X_proc.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6520574",
   "metadata": {},
   "source": [
    "## Entrenamiento y validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83188ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, pandas as pd, joblib\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "y = pd.read_parquet(INTERIM.replace(\".parquet\", \"_y.parquet\")).iloc[:,0]\n",
    "X = np.load(INTERIM.replace(\".parquet\", \"_X.npy\"))\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "model = RandomForestClassifier(n_estimators=300, max_depth=12, random_state=RANDOM_STATE)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=cv, scoring=\"f1_macro\")\n",
    "print(f\"CV f1_macro: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "\n",
    "model.fit(X, y)\n",
    "Path(\"models\").mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(model, MODEL_PATH)\n",
    "MODEL_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16862c",
   "metadata": {},
   "source": [
    "## Evaluación (accuracy, F1 macro, ROC-AUC si aplica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca855d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import json\n",
    "\n",
    "model = joblib.load(MODEL_PATH)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": float(accuracy_score(y, y_pred)),\n",
    "    \"f1_macro\": float(f1_score(y, y_pred, average=\"macro\")),\n",
    "}\n",
    "\n",
    "# AUC si el modelo soporta predict_proba\n",
    "try:\n",
    "    y_prob = model.predict_proba(X)[:,1]\n",
    "    metrics[\"roc_auc\"] = float(roc_auc_score(y, y_prob))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "cm = confusion_matrix(y, y_pred).tolist()\n",
    "metrics[\"confusion_matrix\"] = cm\n",
    "\n",
    "Path(\"reports\").mkdir(parents=True, exist_ok=True)\n",
    "with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f8382c",
   "metadata": {},
   "source": [
    "\n",
    "## Versionado de datos con DVC (guía rápida)\n",
    "\n",
    "```bash\n",
    "git init\n",
    "dvc init\n",
    "\n",
    "# Rastrear dataset \"raw\"\n",
    "dvc add data/raw/german_credit_cleaned_v1.csv\n",
    "git add data/raw/*.dvc .gitignore\n",
    "git commit -m \"Track raw cleaned dataset with DVC\"\n",
    "\n",
    "# Ejecutar pipeline (si usas scripts y dvc.yaml)\n",
    "dvc repro\n",
    "\n",
    "# Ver métricas\n",
    "dvc metrics show\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79757983",
   "metadata": {},
   "source": [
    "\n",
    "## Siguientes pasos / Evidencias para PDF y Video\n",
    "- Insertar capturas de `reports/figures/*` (histogramas, etc.).  \n",
    "- Pegar la tabla de métricas generada (`reports/metrics.json`).  \n",
    "- Añadir interpretabilidad (SHAP) si se requiere explicabilidad adicional.  \n",
    "- Documentar roles, commits y liga al video dentro del PDF final.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
