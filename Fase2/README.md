# Fase 2: German Credit Risk Prediction

[![Python 3.12.1](https://img.shields.io/badge/python-3.12.1-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Proyecto acadÃ©mico de Machine Learning para predecir riesgo crediticio utilizando el dataset German Credit, estructurado siguiendo mejores prÃ¡cticas de MLOps con Cookiecutter Data Science.

**Equipo:** Team 34  
**Curso:** Machine Learning  
**Fecha:** Octubre 2025

---

## ğŸ“‹ Tabla de Contenidos

- [DescripciÃ³n del Proyecto](#descripciÃ³n-del-proyecto)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [InstalaciÃ³n](#instalaciÃ³n)
- [Uso](#uso)
- [Pipeline Completo](#pipeline-completo)
- [Resultados](#resultados)
- [TecnologÃ­as](#tecnologÃ­as)
- [Contribuidores](#contribuidores)

---

## ğŸ¯ DescripciÃ³n del Proyecto

Este proyecto implementa un sistema de predicciÃ³n de riesgo crediticio utilizando el dataset **German Credit**. El objetivo es clasificar clientes como "buen crÃ©dito" (1) o "mal crÃ©dito" (0) basÃ¡ndose en caracterÃ­sticas demogrÃ¡ficas y financieras.

**CaracterÃ­sticas principales:**
- âœ… Pipeline de ML completo y modular
- âœ… Estructura estandarizada con Cookiecutter Data Science
- âœ… CÃ³digo reutilizable y mantenible
- âœ… Versionamiento de modelos y experimentos
- âœ… DocumentaciÃ³n completa

**MÃ©trica objetivo:** AUC-ROC â‰¥ 0.75

---

## ğŸ“ Estructura del Proyecto
```
Fase2/
â”œâ”€â”€ LICENSE              # Licencia MIT
â”œâ”€â”€ Makefile             # Comandos de automatizaciÃ³n
â”œâ”€â”€ README.md            # Este archivo
â”œâ”€â”€ pyproject.toml       # ConfiguraciÃ³n del proyecto
â”œâ”€â”€ requirements.txt     # Dependencias Python
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/             # Datos originales (inmutables)
â”‚   â”œâ”€â”€ interim/         # Datos intermedios (limpiados)
â”‚   â”œâ”€â”€ processed/       # Datos finales para modelado
â”‚   â””â”€â”€ external/        # Datos de terceros
â”‚
â”œâ”€â”€ docs/                # DocumentaciÃ³n con mkdocs
â”‚
â”œâ”€â”€ models/              # Modelos entrenados (.pkl) y metadata
â”‚
â”œâ”€â”€ notebooks/           # Jupyter notebooks de exploraciÃ³n
â”‚   â”œâ”€â”€ 1.0-t34-data-exploration.ipynb
â”‚   â”œâ”€â”€ 2.0-t34-feature-engineering.ipynb
â”‚   â””â”€â”€ 3.0-t34-model-training.ipynb
â”‚
â”œâ”€â”€ references/          # Diccionarios de datos, manuales
â”‚
â”œâ”€â”€ reports/             # AnÃ¡lisis generados (HTML, PDF)
â”‚   â””â”€â”€ figures/         # GrÃ¡ficos y visualizaciones
â”‚
â””â”€â”€ fase2/               # CÃ³digo fuente del proyecto
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ config.py        # ConfiguraciÃ³n global
    â”œâ”€â”€ dataset.py       # Carga y limpieza de datos
    â”œâ”€â”€ features.py      # Feature engineering
    â”œâ”€â”€ plots.py         # Visualizaciones
    â””â”€â”€ modeling/
        â”œâ”€â”€ train.py     # Entrenamiento de modelos
        â””â”€â”€ predict.py   # Inferencia y evaluaciÃ³n
```

---

## ğŸš€ InstalaciÃ³n

### Prerrequisitos

- Python 3.12.1+
- pip
- virtualenv (recomendado)

### Pasos

1. **Clonar el repositorio:**
```bash
   git clone <url-del-repo>
   cd Fase2
```

2. **Crear entorno virtual:**
```bash
   python -m venv .venv
   source .venv/bin/activate  # En Windows: .venv\Scripts\activate
```

3. **Instalar dependencias:**
```bash
   pip install -r requirements.txt
```

4. **Instalar el paquete en modo desarrollo:**
```bash
   pip install -e .
```

5. **Verificar instalaciÃ³n:**
```bash
   python -c "from fase2 import config; print('âœ“ Installation successful')"
```

---

## ğŸ’» Uso

### Pipeline Completo

Ejecuta el pipeline completo de ML en orden:

#### 1ï¸âƒ£ Limpieza de datos
```bash
python -m fase2.dataset
```
**Input:** `data/raw/german_credit_modified.csv`  
**Output:** `data/interim/german_credit_cleaned.csv`

#### 2ï¸âƒ£ Feature Engineering
```bash
python -m fase2.features
```
**Input:** `data/interim/german_credit_cleaned.csv`  
**Output:** 
- `data/processed/X_train.csv`
- `data/processed/X_test.csv`
- `data/processed/y_train.csv`
- `data/processed/y_test.csv`
- `models/scaler.pkl`

#### 3ï¸âƒ£ Entrenamiento de Modelos

**Random Forest:**
```bash
python -m fase2.modeling.train --model-name random_forest
```

**Logistic Regression:**
```bash
python -m fase2.modeling.train --model-name logistic_regression
```

**Decision Tree:**
```bash
python -m fase2.modeling.train --model-name decision_tree
```

**Output:** `models/<model_name>.pkl` + metadata JSON

#### 4ï¸âƒ£ EvaluaciÃ³n
```bash
python -m fase2.modeling.predict --model-path models/random_forest.pkl
```
**Output:** 
- `data/processed/test_predictions.csv`
- `data/processed/test_metrics.json`

#### 5ï¸âƒ£ Visualizaciones
```bash
python -m fase2.plots
```
**Output:** GrÃ¡ficos en `reports/figures/`

---

### Uso desde Notebooks

Los notebooks en `notebooks/` demuestran el uso interactivo:
```python
from fase2.dataset import load_raw_data, translate_columns
from fase2.features import prepare_features
from fase2.modeling.train import train_random_forest

# Cargar y preparar datos
df = load_raw_data()
df = translate_columns(df)
X_train, X_test, y_train, y_test, scaler = prepare_features(df)

# Entrenar modelo
model = train_random_forest(X_train, y_train)
```

---

### Uso como LibrerÃ­a Python
```python
from fase2.dataset import load_raw_data, clean_whitespace
from fase2.features import scale_features
from fase2.modeling.predict import load_model, evaluate_model

# Pipeline personalizado
df = load_raw_data("data/raw/german_credit_modified.csv")
df = clean_whitespace(df)
# ... mÃ¡s transformaciones

# Evaluar modelo guardado
model = load_model("models/random_forest.pkl")
metrics = evaluate_model(model, X_test, y_test)
```

---

## ğŸ“Š Resultados

### Performance de Modelos

| Modelo                | Accuracy | Precision | Recall | F1-Score | AUC-ROC |
|-----------------------|----------|-----------|--------|----------|---------|
| Random Forest         | 0.7543   | 0.7421    | 0.8312 | 0.7841   | **0.7821** |
| Logistic Regression   | 0.7412   | 0.7201    | 0.7945 | 0.7556   | 0.7623  |
| Decision Tree         | 0.7298   | 0.7103    | 0.7812 | 0.7441   | 0.7456  |

âœ… **Modelo seleccionado:** Random Forest (AUC-ROC = 0.7821 > 0.75)

### Features MÃ¡s Importantes

1. `checking_account` (0.142)
2. `duration` (0.118)
3. `amount` (0.095)
4. `age` (0.087)
5. `credit_history` (0.076)

*Ver mÃ¡s detalles en `reports/feature_importance.csv`*

---

## ğŸ› ï¸ TecnologÃ­as

- **Python 3.12.1**
- **LibrerÃ­as principales:**
  - pandas 2.1.0
  - numpy 1.24.3
  - scikit-learn 1.3.0
  - matplotlib 3.7.2
  - seaborn 0.12.2
- **Herramientas:**
  - Cookiecutter Data Science
  - loguru (logging)
  - typer (CLI)
  - joblib (serializaciÃ³n)

---

## ğŸ“ˆ Siguientes Pasos

- [ ] Implementar ensemble methods (XGBoost, LightGBM)
- [ ] Agregar tracking con MLflow
- [ ] Implementar CI/CD pipeline
- [ ] Crear API REST con FastAPI
- [ ] Dockerizar el proyecto
- [ ] Agregar tests unitarios

---

## ğŸ‘¥ Contribuidores

**Team 34**
- Alejandro DÃ­az VillagÃ³mez

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

---

## ğŸ™ Agradecimientos

- Dataset: [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data))
- Template: [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/)

---

## ğŸ“ Contacto

Para preguntas sobre el proyecto, contactar a: team34@example.com

---

**Nota:** Este es un proyecto acadÃ©mico con fines educativos.